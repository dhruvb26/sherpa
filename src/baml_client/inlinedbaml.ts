/*************************************************************************************************

Welcome to Baml! To use this generated code, please run one of the following:

$ npm install @boundaryml/baml
$ yarn add @boundaryml/baml
$ pnpm add @boundaryml/baml

*************************************************************************************************/

// This file was generated by BAML: do not edit it. Instead, edit the BAML
// files and re-generate this code.
//
/* eslint-disable */
// tslint:disable
// @ts-nocheck
// biome-ignore format: autogenerated code
const fileMap = {
  
  "clients.baml": "// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\n\nclient<llm> CustomGPT4o {\n  provider openai\n  options {\n    model \"gpt-4o\"\n    max_tokens 16384\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomGPT4oMini {\n  provider openai\n  retry_policy Exponential\n  options {\n    model \"gpt-4o-mini\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomSonnet {\n  provider anthropic\n  options {\n    model \"claude-3-5-sonnet-20241022\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n\nclient<llm> CustomHaiku {\n  provider anthropic\n  retry_policy Constant\n  options {\n    model \"claude-3-haiku-20240307\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/round-robin\nclient<llm> CustomFast {\n  provider round-robin\n  options {\n    // This will alternate between the two clients\n    strategy [CustomGPT4oMini, CustomHaiku]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/fallback\nclient<llm> OpenaiFallback {\n  provider fallback\n  options {\n    // This will try the clients in order until one succeeds\n    strategy [CustomGPT4oMini, CustomGPT4oMini]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/retry\nretry_policy Constant {\n  max_retries 3\n  // Strategy is optional\n  strategy {\n    type constant_delay\n    delay_ms 200\n  }\n}\n\nretry_policy Exponential {\n  max_retries 2\n  // Strategy is optional\n  strategy {\n    type exponential_backoff\n    delay_ms 300\n    mutliplier 1.5\n    max_delay_ms 10000\n  }\n}",
  "generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"typescript\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.65.0\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode async\n}\n",
  "paper.baml": "class Paper {\n    nodes Node[]\n\n}\n\nclass Node {\n    id string\n    type \"custom\"\n    position Position\n    data NodeData\n    style Style\n}\n\nclass Style {\n    backgroundColor \"hsl(0, 0%, 100%)\"\n    color \"hsl(0, 0%, 0%)\"\n    borderRadius \"0.5rem\"\n}\n\nclass Position {\n    x float\n    y float\n}\n\nclass NodeData {\n    title string\n    description string\n}\n\n\n\nfunction ExtractPaper(prompt: string) -> Paper {\n  client \"openai/gpt-4o\"\n  prompt #\"\n    Extract relevant information from the following paper:\n\n    Position should be decided by the model considering the content and the output format.\n\n    id should be incremented by 1 for each node.\n    {{ prompt }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n\ntest extract_paper {\n  functions [ExtractPaper]\n  args {\n    prompt #\"\n        CoMAL: Collaborative Multi-Agent Large Language Models for\nMixed-Autonomy Traffic\n\narXiv:2410.14368v1 [cs.AI] 18 Oct 2024\n\nHuaiyuan Yao1 , Longchao Da1 , Vishnu Nandam1 , Justin Turnau1 ,\nZhiwei Liu2 , Linsey Pang2 , Hua Wei1\n1\nArizona State University, 2 Salesforce\nAbstract\nThe integration of autonomous vehicles into urban traffic has\ngreat potential to improve efficiency by reducing congestion\nand optimizing traffic flow systematically. In this paper, we\nintroduce CoMAL (Collaborative Multi-Agent LLMs), a\nframework designed to address the mixed-autonomy traffic\nproblem by collaboration among autonomous vehicles to optimize traffic flow. CoMAL is built upon large language\nmodels, operating in an interactive traffic simulation environment. It utilizes a Perception Module to observe surrounding agents and a Memory Module to store strategies\nfor each agent. The overall workflow includes a Collaboration Module that encourages autonomous vehicles to discuss the effective strategy and allocate roles, a reasoning\nengine to determine optimal behaviors based on assigned\nroles, and an Execution Module that controls vehicle actions using a hybrid approach combining rule-based models. Experimental results demonstrate that CoMAL achieves\nsuperior performance on the Flow benchmark. Additionally, we evaluate the impact of different language models\nand compare our framework with reinforcement learning approaches. It highlights the strong cooperative capability\nof LLM agents and presents a promising solution to the\nmixed-autonomy traffic challenge. The code is available at\nhttps://github.com/Hyan-Yao/CoMAL\n\n1\n\nIntroduction\n\nRecently, there has been significant growth in end-toend autonomous driving systems [1]. The integration\nof large language models (LLMs) [2, 3] enhances the\nability to generalize to unseen traffic scenarios with\nembedded common-sense knowledge. These models [4,\n5, 6] primarily focus on optimizing the performance of\nindividual ego vehicles. However, it also matters to\nstudy the problem of mixed-autonomy traffic to better\ndeploy autonomous vehicles in society [7]. In this article,\nwe enable connected autonomous vehicles (CAVs) to\ncollaborate with human-driven vehicles across the traffic\nnetwork, aiming to optimize overall traffic flow and\nsystem-wide efficiency.\n\nTraffic dynamics are extremely complex and chaotic\ndynamical systems [8]. Pioneering researchers [9, 10]\ntypically trade away the complexity of the model for desirable provable properties. To this end, they propose\na series of hand-designed control rules [11, 12] to direct\na fleet of vehicles to form a desired stable motion pattern. The experiments [13, 14] suggest that autonomous\nvehicles can enhance traffic throughput, which highlights the potential of mixed-autonomy systems. To\nmodel the complex interactions between autonomous\nand human-driven vehicles in mixed-autonomy traffic\nsystems, simulation-based numerical methods like the\nFlow benchmark [7, 15] have proven to be effective tools.\nFlow, a deep reinforcement learning (RL) framework,\nenables the systematic design of RL tasks to improve\noverall traffic flow, including the control of autonomous\nvehicles and traffic signals. With its ability to simulate dynamic multi-agent environments, Flow has facilitated the rise of multi-agent RL, which has gained\npopularity for modeling emergent behaviors in mixedautonomy traffic systems [16]. By allowing autonomous\nvehicles to learn cooperative strategies, multi-agent RL\nenables interaction with both human-driven and other\nautonomous vehicles, optimizing traffic flow. These RLbased models demonstrate strong performance in specific traffic scenarios, effectively learning control policies\nfor coordinating mixed-autonomy traffic [17, 18]. However, human behaviors in real-world settings are highly\nvaried and unpredictable, while road networks are more\ncomplex and diverse. Deep RL performs well in specific\nand controlled environments but struggles to generalize\nacross different scenarios [19] due to its reliance on large\namounts of specific training data. Meanwhile, it is difficult to understand the decision-making process of RL,\nwhich limits its interpretability.\nRethinking human behavioral patterns, adolescents\ncan learn to drive in just 20 hours and handle unfamiliar situations [20], while young children can spontaneously cooperate to enhance work efficiency [21]. Human decision-making and cooperation are inherently\n\n\nEnvironment\n\nMultiple Agents on CAVs\nCAV-3\n\nCAV\n\nCAV-2\n\nCAV-4\n\n.....\n\n.\nRing\n\nShared Message Pool\n\nHuman\n\nCAV-n\n\nCAV-1\n\nObs\nScenario\nDescription\n\nActions\nTask\nAllocation\n\nCollaboration Module\n\nPlanner\nGeneration\n\nShared Messages\nSystem prompt\n\nFigure Eight\nReason Engine\n\nLLM Agent\n\nFew-shot\nExperiences\n\nMemory Module\n\nPrompt Generator\n\nUpdates\n\n(a) Single-Agent Pipeline\n\nExecution Module\n\n(b) Multi-Agent Workflow\n\nMerge\n(c) Scenarios\n\nFigure 1: The overall framework of CoMAL. (a) Single-Agent Pipeline: The prompt generator integrates scenario\ndescriptions, few-shot experiences, and shared messages, which are then fed into the LLM. The LLM subsequently\nallocates tasks and generates planners. (b) Multi-Agent Workflow comprises three modules: the Collaboration\nModule, the Reason Engine, and the Execution Module. (c) The three Benchmarks Scenarios for CoMAL Ring:\nThe ring road network consists of a closed-loop road where vehicles continuously travel in a circular fashion.\nFigure Eight (FE): is an extension of the ring road, consisting of two circular loops connected by an intersection.\nMerge: The merged network simulates how vehicles entering from an on-ramp cause disturbances.\nknowledge-driven, relying on common sense, verbal\ncommunication, and reasoning [22]. This is in contrast to data-driven methods that require vast amounts\nof training data and often struggle with generalization. Recent advancements in LLMs [23] offer promising\nknowledge-driven solutions for addressing the mixedautonomy traffic problem. LLMs can emulate humanlike knowledge, including the ability to make decisions,\nform agreements, analyze road situations, and collaborate in real-time. By leveraging this embodied human\nknowledge, multiple intelligent LLM agents with distinct roles and attributes can work together to handle\ncomplex tasks more efficiently [24], offering new avenues\nfor improving traffic systems through cooperation and\ncommon-sense reasoning.\nBuilding upon these insights and recognizing the\nlimitations of RL in generalizing to new traffic situations, we propose leveraging LLMs for their commonsense reasoning and adaptability. LLMs offer better\ngeneralization by drawing on embedded knowledge, allowing them to manage complex and unpredictable traffic dynamics more effectively. We developed an innovative framework named CoMAL specifically designed to\naddress mixed-autonomy traffic challenges, as depicted\nin Figure 1. CoMAL comprises a simulation environment that enables the interaction of individual LLM\nagents and allows agents to collect perceptual data from\n\nthe traffic system. Leveraging the stored experiences\nwithin the Memory Module, the agents participate in\na brainstorming session in the Collaboration Module,\nwhere they allocate tasks and establish their specific\nroles. Subsequently, each agent develops driving plans\naccording to its designated role through the Reason Engine. Then Reason Engine generates a rule-based driving planner grounded in the Intelligent Driver Model\n(IDM), which is then implemented in the Execution\nModule to compensate for the inherent limitations of\nLLMs in control performance.\nOur primary contributions are as follows:\n1) To the best of our knowledge, we are the first to integrate the collaborative capability of multi-agent\nlanguage models in autonomous driving. The Collaboration Module effectively establishes a session\nfor multiple agents to engage in brainstorming and\ntask allocation.\n2) We propose CoMAL, a multi-agent framework designed to address mixed-autonomy traffic challenges. CoMAL integrates rule-based planners to\nenhance control mechanisms while leveraging the\ncooperative and reasoning ability of LLM.\n3) We evaluate CoMAL on Flow benchmark in three\nclassical traffic scenarios (Figure 1 (c)) and com-\n\n\npare its performance against RL methods. Extensive experimental results demonstrate significant\nperformance improvements in terms of average velocity and driving smoothness across various LLMs.\n4) We conduct experiments using the latest LLM\nmodels, including GPT-4o-mini and Qwen72B/32B/7B, showcasing the adaptability of\nCoMAL across a diverse range of LLMs and\ndemonstrating its superior performance over\nstate-of-the-art RL approaches in mixed-autonomy\ntraffic scenarios.\n2\n\nRelated Work\n\n2.1 Mixed-Autonomy Traffic\nMixed-autonomy traffic, where connected autonomous\nvehicles (CAVs) along with human-driven vehicles exist\nin a system [7], presents a significant challenge in\ntraffic dynamics modeling and control. A control\nstrategy named the “slow-in, fast-out” approach [13]\nhas demonstrated improvements in traffic throughput\nwith a minimal percentage of autonomous vehicles.\nReinforcement learning (RL) offers a more dynamic\nand adaptable solution. Benchmarks in RL [25] like\nMujoco and the Arcade Learning Environment [26]\nprovide systematic evaluation and comparison of algorithms. And especially for Mixed-Autonomy, benchmark Flow [7] proposes four traffic scenarios to illustrate distinct RL problems including shockwave\nminimization, inflow management, efficient merging,\nand intersection control. It evaluates and compares\nRL algorithms like Trust Region Policy Optimization (TRPO) [15, 27], Proximal Policy Optimization\n(PPO) [28], Evolutionary Strategies (ES) [29], and Augmented Random Search (ARS) [30] in traffic scenarios. [31] also explores the mixed-autonomy scenario in a\nmulti-agent traffic signal control system [32].\n2.2 Large Language Model-based Multi-Agents\nLarge Language models (LLMs) have become integral\nto multi-agent systems [23] due to their capabilities\nin generalization and common-sense reasoning. LLMbased multi-agent systems leverage these strengths to\nenhance decision-making and communication among\nagents [33]. This approach is particularly beneficial in\ncomplex scenarios like mixed-autonomy traffic, where it\nis essential to have effective interaction between human\ndrivers and autonomous systems.\nThe communication structure of LLM-based multiagent systems varies across different studies to address specific challenges [23, 34, 24]. For example,\nresearch [24] has explored both centralized and decentralized communication structures for LLM-based\n\nmulti-agent systems. In traffic control, LLMs facilitate human-machine interaction and improve decisionmaking processes. The survey [23] discusses how LLMs\ncan be integrated into multi-agent systems to enhance\ncommunication and coordination among agents in traffic environments. And the study DiLu [22] explores\na knowledge-driven approach using LLMs, while LLMLight [35] utilizes LLMs as decision-making agents for\ntraffic signal control. A pioneering model Open-TI [36]\nthat integrates LLMs with external traffic analysis tools\nto perform comprehensive traffic simulations and taskspecific operations. Their findings suggest that incorporating domain-specific knowledge into LLMs can significantly enhance the performance and reliability of multiagent systems in complex scenarios. This involves not\nonly improving the performance of individual agents but\nalso fostering effective communication and coordination\namong the agents.\n3 Methodology\nWe introduce CoMAL, a framework designed for LLM\nagents integrated into connected autonomous vehicles\n(CAVs) to collaborate and enhance the overall velocity\nand driving smoothness of traffic flow. As illustrated\nin Figure 1, we delineate CoMAL at two distinct levels:\nthe single-agent pipeline and the multi-agent workflow.\nAt the single-agent level, the LLM-based agents\nmake decisions based on prompts that include few-shot\nexperiences stored in memory, as well as scenario descriptions derived from environmental perception. The\nagents operate in two modes: task allocation within the\nCollaboration Module and planner generation within\nthe Reason Engine. The multi-agent workflow consists\nof three modules: the Collaboration Module, the Reason Engine, and the Execution Module. In the Collaboration Module, CoMAL establishes a shared message pool that facilitates brainstorming and collaborative decision-making among agents. Within this shared\nspace, agents collectively allocate tasks, define their respective roles, and formulate individual driving plans.\nEach agent then generates a rule-based driving planner in Reason Engine, which is subsequently executed\nwithin the Execution Module to ensure coordinated\ndriving behavior and smooth traffic flow.\n3.1 Single-Agent Pipeline\nThe quality of prompts significantly influences the output quality of LLM. CoMAL utilizes a prompt generator that integrates all essential information for effective decision-making. The workflow for each individual agent involves several steps: (1) encode the scenario into a textual description within the Perception\nModule; (2) recall relevant driving experiences from the\n\n\nDriving Task\nYou are the brain of an autonomous vehicle in the road. Your\nvehicle ID is CAV_01. You can connect all the autonomous\nvehicles in the scenario. Please make the decision to optimize the\naverage velocity of all the vehicles. Try your best to avoid collisions\nwith other objects.\nMap description\nThe merge map consists of two straight single lanes. One is the\nmain highway where all vehicles drive, and another is the merge\nbranch to intersect with main highy way. The perturbations caused\nby the vehicles entering through the ramp leads to the formation of\ncongested waves downstream in the main highway.\nMotion state\nYour speed is 3.80 m/s, and lane position is 11.75 m.\nThere are other vehicles driving around you, and below is their\nbasic information:\n- Vehicle CAV_02 is driving on the same lane as you. The speed of\nit is 4.51 m/s, and lane position is 15.64 m. .........\n\nCollaboration Module\n\nThank you all for your cooperation. Since everyone has confirmed their\npositions and roles, I will lead the queue, setting a consistent speed to\nmaintain safe distances. Let's proceed safely and ensure we avoid\ncollisions while maintaining optimal speed. Drive safely, everyone!\nMy Role Decision: \"Leader\"\n\nReason Engine\nStep 1. Role Clarification\n\nAfter reviewing the positions the consensus shown by other vehicles, I\nam ready to take on the role of the leader.\n\nStep 2. Scene Understanding\n\nMy position at 27.26m provides a good overview ahead and control over\ninitiating movement through the intersection.\n\nStep 3. Motion Instruction\n\nI propose we start moving slowly and steadily to ensure all vehicles can\nfollow safely.\n\nStep 4. Planner Generation\n\nLet’s use the planner: driving_planner(a_max = 0.2, v_target = 6.0,\nmin_headway = 2.0)\n\nFigure 2: (a) Left: A detailed prompt example for CoMAL, consisting of a system prompt that specifies the\ndriving task, along with map description and motion state provided by the Perception Module. (b) Right: A case\nof the collaboration and reasoning process. Following task allocation during brainstorming, a hierarchical chain\nof thought breaks down the driving plan into incremental steps, ensuring consistency in decision-making. This\nprocess includes role clarification, scene understanding, motion instruction, and planner generation.\nMemory Module; (3) receive shared messages from other\nagents through the Collaboration Module; (4) generate prompts and feed it into the LLM; (5) decode the\nLLM response for task allocation or planning purposes.\nIn this section, we detail the Environment Perception\nModule and Memory Module.\n3.1.1 Environment Perception Module To efficiently extract prompts from complex environmental\ndata and enhance the scene understanding of LLMs, we\ndesign an Environment Perception Module. This module extracts key information from the simulation environment and constructs a textual scenario description.\nThe description follows a set of standard rules to generate a thorough representation in natural language. The\nscene information is divided into two parts: static map\nand dynamic agents, as shown in Figure 2.\nThe static map information represents the scenario\ntype, providing semantic priors for vehicle motion planning. The description of the map helps the LLM intuitively understand the scenario’s geometry. The dynamic information describes the motion of the ego ve-\n\nhicle and surrounding agents, which directly influences\nthe planning of vehicles’ movement.\n3.1.2 Memory Module Similar to human drivers,\nthe agent must make decisions based on reasoning\nprocesses that are informed by past driving experiences.\nTo achieve this, we employ a Memory Module that\nstores experiences from previous driving scenarios and\nhandmade instructions. Initially, the agent is provided\nwith a set of predefined experiences, which the LLM\nthen updates continuously as it engages in reasoning\nduring new situations. This approach allows the agent\nto refine its decision-making over time, improving its\nperformance in diverse driving contexts.\n3.2 Multi-Agent Workflow In a mixed-autonomy\ntraffic setting, where CAVs operate alongside human\ndrivers, the main objective is to optimize overall traffic\nflow. To achieve this, we propose a three-stage decisionmaking workflow. In the Collaboration Module, agents\nfirst discuss and allocate tasks within a public message\npool. In Reason Engine, each agent then independently\n\n\nCAV-01\n\nCAV-02\n\nGood day, fellow vehicles! This is Vehicle 01. In order to form an efficient queue that allows us all to pass quickly\nand safely, I propose the following initial order according to our current positions: Vehicle 1, Vehicle 2, Vehicle 3,\nVehicle 4 ...... Each vehicle should maintain a safe following distance from the vehicle in front. Please confirm your\nagreement or suggest adjustments. Let's ensure we avoid collisions and maintain optimal speed!\nMy Role Decision: \"Not decided yet\"\n\nHello everyone, this is Vehicle 02. I agree with Vehicle 01's proposed order since it reflects our current positions\naccurately. If no one has any objections, I suggest we move forward with this plan and start adjusting our speeds to\nmaintain safe distances. Waiting for confirmation from the rest. Thank you!\nMy Role Decision: \"Follower\"\n\n......\n\nCAV-01\n\nThank you all for your cooperation. Since everyone has confirmed their positions and roles, I will lead the queue,\nsetting a consistent speed to maintain safe distances. Here's the final order and role assignment: 1. Vehicle 1\n(Leader) 2. Vehicle 2 (Leading Follower) 3. Vehicle 3 (Follower).... Let's proceed safely and ensure we avoid\ncollisions while maintaining optimal speed. Drive safely, everyone!\nMy Role Decision: \"Leader\"\n\nFigure 3: Demonstration of the interaction process of agents in the scenario Figure Eight 1. The agents decide\nto form a queue and subsequently allocate the roles of leader and follower.\nSystem Prompt The system prompt defines the\ndetermines its driving plan based on its assigned role\nand generates a driving planner. Finally, the driving planning task and associated driving knowledge. Its primary goal is to standardize the format of both input and\nplanner is executed in the Execution Module.\noutput data, as well as clarify the objectives of planner\n3.2.1 Collaboration Module Collaborative agents generation. Specifically, it ensures a clear understanding\nwork together towards a shared objective, typically ex- of the physical meaning of each parameter in the IDM\nchanging information to enhance the collective solution. planner, such as speed limit (v0 ), maximum acceleraIn the Collaboration Module, all CAVs participate in- tion (am ), and minimum headway (s0 ). This provides a\nteractively by forming a queue for brainstorming and structured foundation for the decision-making process.\nHierarchical Chain-of-thougts The hierarchical\ncommunication. In the brainstorming session, the vehicles take turns subsequently speaking in a public chan- chain-of-thought process involves four critical componel to propose strategies and assign tasks among them- nents: role clarification, scene understanding, motion\nselves. This collaborative effort ensures that each CAV instruction, and planner generation. Initially, it is crucial to clarify the role and task of the ego vehicle within\nunderstands its specific role in the traffic system.\nCommunication Structure Here, we introduce a a collaborative context. Then the LLM is directed to\nshared message pool to boost communication efficiency, focus on key information in the scenario, such as headas shown in Figure 3. This communication structure way distance and lead vehicles. Based on the scenario\nmaintains a shared message pool where agents can analysis, the LLM is then prompted to provide motion\npublish messages and subscribe to the latest messages instructions for the ego vehicle. Finally, each agent utifrom one another. Agents will take turns to speak one lizes scenario analysis and motion instructions to genat a time until the strategy is fully developed and each erate a driving planner, parameterized by IDM model.\nagent’s role is clearly defined.\n3.2.3 Execution Module We utilize the rule-based\n3.2.2 Reason Engine During team brainstorming, IDM model as a planner to execute driving strategies by\neach agent determines its role and formulates a strat- adjusting its parameters. IDM is a car-following model\negy to collaborate with other vehicles. Based on these to compute longitudinal dynamics. In this model, the\ndefined roles, the Reason Engine generates an appro- acceleration ak for vehicle k is defined by its bumperpriate driving planner to effectively control the vehi- to-bumper headway sk (distance to preceding vehicle),\ncle. The Reason Engine takes scenario description and velocity vk , and relative velocity ∆vk , via the following\npredefined system prompts as inputs. Subsequently, equation:\nthe LLM generates the driving planner based on IDM\nthrough reasoning guided by a hierarchical chain-ofthought prompt.\n\n\n(3.1)\n\nak =\n\nvk\ndvk\ns∗ (vk , ∆vk ) 2\n= amax [1 − ( )δ − (\n) ]\ndt\nv0\nsk\n\nwhere s∗ is the desired headway of the vehicle,\ndenoted by:\n\n(3.2)\n\nvk ∆vk\n)\ns∗ (vk , ∆vk ) = s0 + max(0, vk T + √\n2 amax b\n\nwhere s0 , v0 , T, δ, amax , b are given parameters. We\nset the desired time headway T , the comfortable braking\ndeceleration b, and the acceleration exponent δ as\nconstants while adjusting the desired velocity v0 , the\nminimum spacing s0 , and the maximum acceleration\namax to tailor the driving planners. Thus Reason\nEngine generates a driving planner by customizing\nIDM’s parameters (v0 , amax , s0 ).\n4\n\nExperiments\n\nIn a mixed-autonomy setting, a subset of vehicles are\ntasked with the objective of improving overall traffic\nflow and mitigating the formation and propagation of\nstop-and-go waves. Thus, in our experiments, we aim\nto address several key questions:\n• How can CAVs enhance traffic flow and eliminate\nstop-and-go shockwaves?\n• How do multiple LLM-based agents collaborate to\nachieve this goal?\n• Do different LLM models influence the results?\n\nFigure Eight (FE) The FE network builds on\nthe ring road by connecting two circular loops via an\nintersection. In mixed-autonomy scenarios, CAVs are\nintroduced to smooth traffic and prevent stop-and-go\nwaves.\nMerge The merged network simulates highway\ndisturbances caused by vehicles entering from an onramp, which creates stop-and-go waves. In mixedautonomy scenarios, CAVs are tasked with mitigating\nthese waves based on local observations and adjusting\nto fluctuating vehicle numbers in the open network.\nWe investigate different levels of difficulty for each\nproposed benchmark by adjusting their scenario-specific\nmeta-parameters. Table 1 provides detailed descriptions\nof the selected meta-parameters for each benchmark.\nTable 1: Configurations of Benchmarks\nScenario Name Time(s)\nVehicles Distribution\nFE 0\nFE 1\nFE 2\nRing 0\nRing 1\nRing 2\nMerge 0\nMerge 1\nMerge 2\nMerge 3\nMerge 4\n\n150\n150\n150\n150\n150\n150\n75\n75\n75\n75\n75\n\n13 humans, 1 CAV\n7 humans, 7 CAVs\n0 humans, 14 CAVs\n21 humans, 1 CAV\n19 humans, 3 CAVs\n11 humans, 11 CAVs\n10.0% CAV penetration rate\n25.0% CAV penetration rate\n33.3% CAV penetration rate\n50.0% CAV penetration rate\n90.0% CAV penetration rate\n\n4.3 Metrics To provide a comprehensive assessment\nof traffic flow and mitigate the occurrence of shockwaves, we utilize two metrics:\n\n4.1 Implementation Details The experiments are\nconducted in Flow [15] with SUMO [37], a microscopic\n• Average vehicle speed in the network (m/s). Higher\nsimulator for traffic and vehicle dynamics. For details\nvalues indicate better overall traffic flow.\non the architecture and on training autonomous vehicles\n• Standard deviation of vehicle speed (m/s). The\nto maximize system-level velocity, we refer the readers\nsmaller is more stable. Lower values reflect greater\nto [15]. The environment offers several driving models\nstability and consistency in traffic movement.\nto simulate human driver and a realistic interaction\nbetween vehicles. We adopt OpenAI GPT-4o-mini,\n4.4 Specification on Communication In this secQwen-72B/32B/7B in this paper.\ntion, we focus on the interactive process among agents\n4.2 Scenarios We evaluate our model on the Figure as they work to solve the mixed-traffic problem. In the\nEight (FE), Ring, and Merge scenarios from the Flow FE scenario, the agents recognize the need to form a\nbenchmark. Further details are provided below and queue, identify a leader, and designate the remaining\nagents as followers. The process of task allocation and\nillustrated in Figure 1 (c).\nRing The ring road network consists of a circu- leader selection is illustrated in Figure 3. Additionally,\nlar lane where vehicles continuously travel in a loop. in the ring and merge scenarios, agents aim to elimiIt is commonly used to study traffic dynamics, as dis- nate shockwaves. Their reasoning is as follows: if there\nturbances can cause stop-and-go waves. In mixed- is relative traffic congestion ahead of the ego vehicle, the\nautonomy scenarios, CAVs are deployed to reduce these agent approaches the lead vehicle slowly; otherwise, it\naccelerates to follow the lead vehicle closely.\nwaves and enhance traffic flow stability.\n\n\nMetric\n\nTable 2: Quantitative Evaluation of CoMAL on Flow Benchmarks\nFE 0 FE 1 FE 2 Ring 0 Ring 1 Ring 2 Merge 0 Merge 1 Merge 2 Merge 3 Merge 4\n\nModel\n\nAvg\n\nHuman Driver 5.61 5.61 5.61\nCoMAL\n6.40 6.47 6.29\n\n2.88\n2.86\n\n2.88\n2.85\n\n2.88\n2.87\n\n6.40\n6.59\n\n6.40\n7.40\n\n6.40\n7.42\n\n6.40\n7.86\n\n6.40\n8.83\n\nStd\n\nHuman Driver 4.55 4.55 4.55\nCoMAL\n1.74 1.77 2.24\n\n0.79\n0.29\n\n0.79\n0.26\n\n0.79\n0.31\n\n3.12\n2.88\n\n3.12\n2.91\n\n3.12\n2.61\n\n3.12\n2.47\n\n3.12\n2.70\n\nFigure 4: Visualization of vehicle trajectories in Ring 0 setting. The ring road has a total length of 230 meters\nand contains 22 vehicles. Each line in the space-time diagrams shows the position of a specific vehicle over time.\nWhen a vehicle completes a full lap of the ring, its position resets to zero. Left: In the absence of automated\nvehicles, human-driven vehicles exhibit stop-and-go shockwaves due to inherent instability. Right: With three\nconnected autonomous vehicles using the CoMAL framework, the unstable vehicles are stabilized.\nTable 3: Ablation Study\nNo. Perception Memory Collaboration FE 1 Merge 1\n1\n2\n3\n4\n5\n\n×\n×\n✓\n✓\n✓\n\n×\n✓\n×\n✓\n✓\n\n×\n✓\n✓\n×\n✓\n\n5.61 6.40\n5.81 6.51\n5.17 6.72\n5.18 6.88\n6.47 7.40\n\n4.5 Quantitative Results We evaluated our model\non the aforementioned benchmarks, varying the number\nor percentage of CAVs across different settings.\nAs shown in Table 2, we compared the performance\nof CoMAL with that of human drivers. The results indicate that as the number of CAVs increases, CoMAL’s\nperformance generally improves and surpasses that of\nhuman drivers. This highlights the strong capability of\nLLM agents in achieving effective cooperation. A visualization of the vehicle trajectories in Ring 0 setting is\nshown in Figure 4. We can see that the proposed CoMAL framework can stabilize the unstable vehicle flow.\n\n4.6 Ablation Studies We conducted a detailed\nanalysis of the effectiveness of each component of CoMAL in the FE 1 and Merge 1 scenarios, as presented\nin Table 3.\nAblation on Perception The comparisons in the\nsecond and fifth rows of Table 3 demonstrate the\neffectiveness of incorporating textual descriptions of\nthe map and agents’ motion states in the Perception\nModule. Once perception information is lost, agents are\nno longer able to comprehend the spatial relationships\nbetween the ego vehicle and surrounding agents. As\na result, their capacity for effective collaboration and\nreasoning is significantly impaired.\nAblation on Memory The comparisons in the\nthird and fifth rows of Table 3 illustrate the impact\nof the Memory Module, in which specific experiences\nare allocated for each scenario. In the absence of\nhigh-quality experiences, agents are more susceptible to\nerrors in both discussion and reasoning.\nAblation on Collaboration The comparison presented in the fourth and fifth rows of Table 3, as well as\nin the first row, highlights the effectiveness of the Collaboration Module. The absence of a collaboration mod-\n\n\nTable 4: Quantitative Experiment of Different LLMs: Average Velocity and Standard Deviation Analysis\nAverage Velocity (m/s)\nModel\nFE 0 FE 1 FE 2 Ring 0 Ring 1 Ring 2 Merge 0 Merge 1 Merge 2 Merge 3 Merge 4\nHuman Driver 5.61\nGPT-4o-mini 6.40\nQwen-72B\n6.37\nQwen-32B\n6.39\nQwen-7B\n5.60\n\n5.61\n6.47\n5.96\n5.73\n5.17\n\n5.61\n6.29\n6.07\n6.11\n4.80\n\n2.88\n2.86\n2.86\n2.84\n2.82\n\n2.88\n2.85\n2.83\n2.86\n2.84\n\n2.88\n2.87\n2.85\n2.81\n2.83\n\n6.40\n6.59\n6.66\n6.58\n6.55\n\n6.40\n7.40\n7.38\n7.12\n6.87\n\n6.40\n7.42\n7.46\n7.39\n7.05\n\n6.40\n7.86\n8.07\n7.73\n7.47\n\n6.40\n8.83\n8.75\n8.54\n8.53\n\nStandard Deviation (m/s)\n\nModel\n\nFE 0 FE 1 FE 2 Ring 0 Ring 1 Ring 2 Merge 0 Merge 1 Merge 2 Merge 3 Merge 4\nHuman Driver 4.55\nGPT-4o-mini\n1.74\nQwen-72B\n1.73\nQwen-32B\n1.74\nQwen-7B\n4.51\n\n4.55\n1.77\n2.76\n2.37\n1.89\n\n4.55\n2.24\n1.92\n2.64\n1.76\n\n0.79\n0.29\n0.32\n0.33\n0.49\n\n0.79\n0.26\n0.32\n0.37\n0.61\n\n0.79\n0.31\n0.30\n0.33\n0.48\n\n3.12\n2.88\n2.82\n2.81\n3.02\n\n3.12\n2.91\n2.71\n2.99\n2.88\n\n3.12\n2.61\n2.54\n2.62\n2.74\n\n3.12\n2.47\n2.61\n2.57\n2.71\n\n3.12\n2.70\n2.78\n2.50\n2.64\n\nComparison of various LLM models We evalTable 5: Comparison to RL Benchmark\nuate performance across LLM models of varying sizes\nModel FE 0 FE 1 FE 2 Merge 0 Merge 1 Merge 2 (see Table 4). The GPT-4o-mini achieves the highest\nIDM\n5.61 5.61 5.61\n6.40\n6.40\n6.40 performance among these. Among open-source models,\nARS\n7.31 6.43 5.70 11.30\n11.06\n11.50 the Qwen 72B has a similar level with the GPT-4o-mini,\nES\n6.87\n5.96 13.31\n17.29\n17.36 while the Qwen 32B shows slightly lower performance,\nTRPO 8.26 5.61 5.03 14.95\n13.74\n14.14 and the Qwen 7B performs significantly worse. Notably,\nPPO\n8.26 5.61 5.03 13.66\n14.61\n14.54 we observe that in scenarios requiring extensive collabCoMAL 6.40 6.47 6.29\n6.59\n7.40\n7.42 oration, the performance of smaller models deteriorates\nmore rapidly. This finding suggests that collaboration\nis a more challenging task than reasoning within the\nCoMAL framework.\nule causes all agents to adopt nearly identical strategies,\nwhich in turn leads to conflicts and duplicated efforts.\nThe absence of collaboration results in performance that\ncan be even worse than that of the simple rule-based\nmodel shown in the first row.\n4.7\n\nDiscussion\nCompared to RL method We conducted experiments on FE and the Merge scenarios by comparing\nthem to RL methods developed in [7]. The results\nare shown in Table 5. The Ring is not included in\nthis experiment because the adopted RL benchmark\ndoesn’t have the Ring scenario embedded. In the FE\nscenario, CoMAL demonstrates robust global collaboration, whereas multi-agent RL models struggle to differentiate roles, hindering effective cooperation. Consequently, CoMAL outperforms RL-based approaches.\nHowever, in the Merge scenario, CoMAL performs less\neffectively than RL, indicating that the collaboration is\nnot global. This finding highlights the critical importance of cooperation in enhancing performance.\n\n5\n\nConclusion\n\nIn this paper, we present CoMAL, an effective\nLLM-based multi-agent framework to address mixedautonomy traffic challenges. By prompt-tuning LLMs\nwith a hierarchical LLM-based planner, CoMAL is able\nto handle complex vehicle driving tasks towards a collaborative goal under mixed-autonomy traffic. The\nLLM agent does so by serving primarily as a high-level\ncommander, coordinating with lower-level controllers\nto execute detailed operations. Extensive experiments\ndemonstrate the significant potential of multi-agent systems driven by LLMs to make informed decisions and\ncollaborate effectively in driving scenarios.\nWe also acknowledge the limitations of our current\nwork and would like to point out several important future directions. First, we can extend the experimental\ncurrent settings to more agents and improve their collaboration to see if there would be emerging behaviors\nformed like in RL methods. Second, our paper addresses\na simplified use of LLMs alone, whereas the combina-\n\n\ntion of RL with LLM might be helpful in improving\nthe performance of LLMs. In the future, more sophisticated scenarios will be explored to test the possibility\nof LLM’s behavior in complex tasks.\nAcknowledgements\nWe thank OpenAI for providing us with API credits\nunder the Researcher Access program.\nReferences\n[1] P. S. Chib and P. Singh, ”Recent Advancements in\nEnd-to-End Autonomous Driving Using Deep Learning: A Survey,” in IEEE Transactions on Intelligent\nVehicles, vol. 9, no. 1, pp. 103-118, Jan. 2024, doi:\n10.1109/TIV.2023.3318070.\n[2] Hallgarten, M., Zapata, J., Stoll, M., Renz, K., Zell,\nA. (2024). Can Vehicle Motion Planning Generalize to Realistic Long-tail Scenarios?. arXiv preprint\narXiv:2404.07569.\n[3] Da, L., Gao, M., Mei, H., Wei, H. (2024). Prompt\nto Transfer: Sim-to-Real Transfer for Traffic Signal\nControl with Prompt Learning. Proceedings of the\nAAAI Conference on Artificial Intelligence, 38(1), 8290. https://doi.org/10.1609/aaai.v38i1.27758\n[4] Y. Hu et al., ”Planning-oriented Autonomous Driving,” 2023 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), Vancouver, BC, Canada, 2023, pp. 17853-17862, doi:\n10.1109/CVPR52729.2023.01712.\n[5] Dauner, D., Hallgarten, M., Geiger, A., Chitta, K.\n(2023, December). Parting with misconceptions about\nlearning-based vehicle motion planning. In Conference\non Robot Learning (pp. 1268-1281). PMLR.\n[6] Mao, J., Qian, Y., Ye, J., Zhao, H., Wang, Y. (2023).\nGpt-driver: Learning to drive with gpt. arXiv preprint\narXiv:2310.01415.\n[7] Vinitsky, E., Kreidieh, A., Flem, L.L., Kheterpal,\nN., Jang, K., Wu, C., Wu, F., Liaw, R., Liang, E.\nBayen, A.M.. (2018). Benchmarks for reinforcement\nlearning in mixed-autonomy traffic. Proceedings of The\n2nd Conference on Robot Learning, in Proceedings of\nMachine Learning Research 87:399-409 Available from\nhttps://proceedings.mlr.press/v87/vinitsky18a.html.\n[8] Chang, T., Lai, I. (1997). Analysis of characteristics of\nmixed traffic flow of autopilot vehicles and manual vehicles. Transportation Research Part C-emerging Technologies, 5, 333-348.\n[9] C. Wu, A. M. Bayen and A. Mehta, ”Stabilizing\nTraffic with Autonomous Vehicles,” 2018 IEEE International Conference on Robotics and Automation\n(ICRA), Brisbane, QLD, Australia, 2018, pp. 60126018, doi: 10.1109/ICRA.2018.8460567.\n[10] M. Papageorgiou, C. Diakaki, V. Dinopoulou, A.\nKotsialos and Yibing Wang, ”Review of road traffic control strategies,” in Proceedings of the IEEE,\n\nvol. 91, no. 12, pp. 2043-2067, Dec. 2003, doi:\n10.1109/JPROC.2003.819610.\n[11] B. Besselink and K. H. Johansson, ”String Stability\nand a Delay-Based Spacing Policy for Vehicle Platoons\nSubject to Disturbances,” in IEEE Transactions on\nAutomatic Control, vol. 62, no. 9, pp. 4376-4391, Sept.\n2017, doi: 10.1109/TAC.2017.2682421.\n[12] SHLADOVER, STEVEN E. 1995. “Review of the State\nof Development of Advanced Vehicle Control Systems\n(AVCS).” Vehicle System Dynamics 24 (6–7): 551–95.\ndoi:10.1080/00423119508969108.\n[13] Taniguchi, Y., Nishi, R., Tomoeda, A., Shimura, K.,\nEzaki, T., Nishinari, K. (2015). A Demonstration Experiment of a Theory of Jam-Absorption Driving. In:\nChraibi, M., Boltes, M., Schadschneider, A., Seyfried,\nA. (eds) Traffic and Granular Flow ’13. Springer,\nCham. https://doi.org/10.1007/978-3-319-10629-853\n[14] S. Mosharafian and J. M. Velni, ”Cooperative Adaptive\nCruise Control in a Mixed-Autonomy Traffic System:\nA Hybrid Stochastic Predictive Approach Incorporating Lane Change,” in IEEE Transactions on Vehicular\nTechnology, vol. 72, no. 1, pp. 136-148, Jan. 2023, doi:\n10.1109/TVT.2022.3202084.\n[15] C. Wu, A. R. Kreidieh, K. Parvate, E. Vinitsky and\nA. M. Bayen, ”Flow: A Modular Learning Framework\nfor Mixed Autonomy Traffic,” in IEEE Transactions on\nRobotics, vol. 38, no. 2, pp. 1270-1286, April 2022, doi:\n10.1109/TRO.2021.3087314.\n[16] Wu, C., Kreidieh, A., Vinitsky, E. Bayen, A.M.\n(2017). Emergent Behaviors in Mixed-Autonomy\nTraffic. Proceedings of the 1st Annual Conference on Robot Learning, in Proceedings of Machine Learning Research 78:398-407 Available from\nhttps://proceedings.mlr.press/v78/wu17a.html.\n[17] R. Valiente, B. Toghi, R. Pedarsani and Y. P. Fallah, ”Robustness and Adaptability of Reinforcement\nLearning-Based Cooperative Autonomous Driving in\nMixed-Autonomy Traffic,” in IEEE Open Journal of\nIntelligent Transportation Systems, vol. 3, pp. 397-410,\n2022, doi: 10.1109/OJITS.2022.3172981.\n[18] Liu, Lu, Maonan Wang, Man-On Pun and Xi Xiong.\n“A Multi-Agent Rollout Approach for Highway Bottleneck Decongenston in Mixed Autonomy.” ArXiv\nabs/2405.03132 (2024): n. pag.\n[19] Glanois, C., Weng, P., Zimmer, M. et al. A survey on\ninterpretable reinforcement learning. Mach Learn 113,\n5847–5890 (2024). https://doi.org/10.1007/s10994024-06543-w\n[20] LeCun, Yann and Courant. “A Path Towards Autonomous Machine Intelligence Version 0.9.2, 2022-0627.” (2022).\n[21] M.A. Veldman, S. Doolaard, R.J. Bosker, T.A.B.\nSnijders, Young children working together. Cooperative learning effects on group work of children in\nGrade 1 of primary education, Learning and Instruction, Volume 67, 2020, 101308, ISSN 0959-4752,\nhttps://doi.org/10.1016/j.learninstruc.2020.101308.\n[22] Wen, L., D. Fu, X. Li, X. Cai, T. Ma, P. Cai, M. Dou,\n\n\nB. Shi, L. He, and Y. Qiao. “DILU: A KNOWLEDGEDRIVEN APPROACH TO AUTONOMOUS DRIVING WITH LARGE LANGUAGE MODELS.” In 12th\nInternational Conference on Learning Representations,\nICLR 2024, 2024.\n[23] Guo, Taicheng, Xiuying Chen, Yaqi Wang, Ruidi\nChang, Shichao Pei, N. Chawla, Olaf Wiest and Xiangliang Zhang. “Large Language Model based MultiAgents: A Survey of Progress and Challenges.” ArXiv\nabs/2402.01680 (2024): n. pag.\n[24] Talebirad, Yashar and Amirhossein Nadiri. “MultiAgent Collaboration: Harnessing the Power of Intelligent LLM Agents.” ArXiv abs/2306.03314 (2023): n.\npag.\n[25] Duan, Y., Chen, X., Houthooft, R., Schulman,\nJ. Abbeel, P. (2016). Benchmarking Deep Reinforcement Learning for Continuous Control.\nProceedings of The 33rd International Conference\non Machine Learning, in Proceedings of Machine\nLearning Research 48:1329-1338 Available from\nhttps://proceedings.mlr.press/v48/duan16.html.\n[26] Bellemare M G, Naddaf Y, Veness J, et al. The\narcade learning environment: An evaluation platform\nfor general agents[J]. Journal of Artificial Intelligence\nResearch, 2013, 47: 253-279.\n[27] John Schulman, Sergey Levine, Philipp Moritz,\nMichael Jordan, and Pieter Abbeel. 2015. Trust region policy optimization. In Proceedings of the 32nd International Conference on International Conference on\nMachine Learning - Volume 37 (ICML’15). JMLR.org,\n1889–1897.\n[28] Schulman J, Wolski F, Dhariwal P, et al. Proximal policy optimization algorithms[J]. arXiv preprint\narXiv:1707.06347, 2017.\n[29] Salimans T, Ho J, Chen X, et al. Evolution strategies\nas a scalable alternative to reinforcement learning[J].\narXiv preprint arXiv:1703.03864, 2017.\n[30] Mania H, Guy A, Recht B. Simple random search provides a competitive approach to reinforcement learning[J]. arXiv preprint arXiv:1803.07055, 2018.\n[31] Hao Mei, Junxian Li, Bin Shi, and Hua Wei, Reinforcement learning approaches for traffic signal control under missing data, Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence,\npp. 2261–2269, 2023.\n[32] Mei, H., Lei, X., Da, L. et al. Libsignal: an open library\nfor traffic signal control. Mach Learn 113, 5235–5271\n(2024). https://doi.org/10.1007/s10994-023-06412-y\n[33] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang,\nHao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang,\nXu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei,\nand Jirong Wen. 2024. A survey on large language\nmodel based autonomous agents. Front. Comput. Sci.\n18, 6 (Dec 2024). https://doi.org/10.1007/s11704-02440231-1\n[34] Masterman T, Besen S, Sawtell M, et al. The landscape of emerging ai agent architectures for reasoning,\nplanning, and tool calling: A survey[J]. arXiv preprint\n\narXiv:2404.11584, 2024.\n[35] Lai S, Xu Z, Zhang W, et al. Large language models\nas traffic signal control agents: Capacity and opportunity[J]. arXiv preprint arXiv:2312.16044, 2023.\n[36] Da L, Liou K, Chen T, et al. Open-ti: Open traffic\nintelligence with augmented language model[J]. International Journal of Machine Learning and Cybernetics,\n2024: 1-26.\n[37] Lopez P A, Behrisch M, Bieker-Walz L, et al. Microscopic traffic simulation using sumo[C]//2018 21st\ninternational conference on intelligent transportation\nsystems (ITSC). IEEE, 2018: 2575-2582.\n\n\n\n    \"#\n  }\n}\n",
  "resume.baml": "// Defining a data model.\nclass Resume {\n  name string\n  email string\n  experience string[]\n  skills string[]\n}\n\n// Create a function to extract the resume from a string.\nfunction ExtractResume(resume: string) -> Resume {\n  // Specify a client as provider/model-name\n  // you can use custom LLM params with a custom client name from clients.baml like \"client CustomHaiku\"\n  client \"openai/gpt-4o\" // Set OPENAI_API_KEY to use this client.\n  prompt #\"\n    Extract from this content:\n    {{ resume }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n\n// Test the function with a sample resume. Open the VSCode playground to run this.\ntest vaibhav_resume {\n  functions [ExtractResume]\n  args {\n    resume #\"\n      Vaibhav Gupta\n      vbv@boundaryml.com\n\n      Experience:\n      - Founder at BoundaryML\n      - CV Engineer at Google\n      - CV Engineer at Microsoft\n\n      Skills:\n      - Rust\n      - C++\n    \"#\n  }\n}\n",
}
export const getBamlFiles = () => {
    return fileMap;
}